{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4370,
     "status": "ok",
     "timestamp": 1638988150623,
     "user": {
      "displayName": "Sude Aydın",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjX2JQVbIPi2KP3QPSkJFDqedasbw2kOgLoilOu=s64",
      "userId": "07115023300107077639"
     },
     "user_tz": -180
    },
    "id": "bOIpLaqrTgAe",
    "outputId": "5dfcc64b-1736-41ed-c51d-2411059c8ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |████                            | 10 kB 22.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 20 kB 25.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 30 kB 31.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 40 kB 15.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 51 kB 6.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 61 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 71 kB 5.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 81 kB 6.3 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 82 kB 335 kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "executionInfo": {
     "elapsed": 1008,
     "status": "error",
     "timestamp": 1638988157489,
     "user": {
      "displayName": "Sude Aydın",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjX2JQVbIPi2KP3QPSkJFDqedasbw2kOgLoilOu=s64",
      "userId": "07115023300107077639"
     },
     "user_tz": -180
    },
    "id": "6n42RHnVSion",
    "outputId": "e038fe8a-64a3-4ad1-cbeb-41164b47facf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cc3476dbf860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#veri seti data frame e yüklenir.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdatadf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"disasters.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#DATA PREPROCESSİNG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'disasters.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#veri seti data frame e yüklenir.\n",
    "datadf=pd.read_csv(\"disasters.csv\")\n",
    "\n",
    "#DATA PREPROCESSİNG\n",
    "\n",
    "#1)Feature engineering\n",
    "#Başlangıç ve bitiş yılları çok büyük oranla aynı olduğundan o sutunları dusurdum ve ay bilgisini kullanıp mevsim adında yeni bir sutun oluşturdum.\n",
    "#Bunun için öncelikle NULL olan değerleri doldurdum\n",
    "datadf.drop([\"Start Day\",\"End Day\",\"End Year\",\"Start Year\",\"End Month\"],axis=1,inplace=True)\n",
    "datadf.fillna(value={\"Start Month\" :datadf[\"Start Month\"].mean().astype(int)},inplace=True)\n",
    "datadf['Start Month'] = datadf['Start Month'].replace([3,4,5],'Spring')\n",
    "datadf['Start Month'] = datadf['Start Month'].replace([6,7,8],'Summer')\n",
    "datadf['Start Month'] = datadf['Start Month'].replace([9,10,11],'Fall')\n",
    "datadf['Start Month'] = datadf['Start Month'].replace([12,1,2],'Winter')\n",
    "datadf.rename(columns={\"Start Month\" :\"Season\"},inplace=True)\n",
    "\n",
    "#Country sutunu ile aynı bilgiyi verdiği için Location sutununu dusurduk\n",
    "datadf.drop([\"Location\"],axis=1,inplace=True)\n",
    "\n",
    "#Disaster Magnitude Value sutunu çok büyük oranda boş olduğundan o sutunun birimini gösteren Dis Mag Scale sutunu da dusurulur.\n",
    "datadf.drop([\"Dis Mag Scale\"],axis=1,inplace=True)\n",
    "\n",
    "#Country sutunu ile aynı olduğundan düşürüldü.\n",
    "datadf.drop([\"ISO\"],axis=1,inplace=True)\n",
    "\n",
    "#Continent sutunu ile çok büyük oranda  aynı bilgiyi verdiği için düşürüldü.\n",
    "datadf.drop([\"Region\"],axis=1,inplace=True)\n",
    "\n",
    "#No effected ve Total effected sutunları aynı bilgiyi bize verdiği  için birini düşürdük. Daha buyuk oranda boş olanı dusurduk.\n",
    "datadf.drop([\"No Affected\"],axis=1,inplace=True)\n",
    "\n",
    "# Tüm satırlardaki değeri aynı olduğundan -Natural- Disaster Group sutunu dusurdum.\n",
    "datadf.drop([\"Disaster Group\"],axis=1,inplace=True)\n",
    "\n",
    "#2)Missing data\n",
    "#doldurulamayacak oranda boş olan columnların veri setinden silinmesi\n",
    "columnstodrop=[\"Glide\",\"Disaster Subsubtype\",\"Event Name\",\"Origin\",\"Associated Dis\",\"Associated Dis2\",\"OFDA Response\",\"Appeal\",\"Declaration\",\n",
    "\"Aid Contribution\",\"Dis Mag Value\",\"Latitude\",\"Longitude\",\"Local Time\",\"River Basin\",\"No Injured\",\n",
    "\"No Homeless\",\"Insured Damages ('000 US$)\",\"Total Damages ('000 US$)\",\"Adm Level\",\"Admin1 Code\",\"Admin2 Code\",\"Geo Locations\"]\n",
    "datadf.drop(columnstodrop,axis=1,inplace=True)   #%51-%96 oranında boş olan columnları veri setinden düşürülür.\n",
    "\n",
    "#NULL değeri olan columnların doldurulması\n",
    "#%19 boş olan kategorik sutunun en çok tekrar eden veri ile doldurulması\n",
    "datadf[\"Disaster Subtype\"].fillna(datadf.groupby('Disaster Subgroup')['Disaster Subtype'].transform(lambda x : next(iter(x.mode()), np.nan)),inplace=True)\n",
    "datadf=datadf.dropna(subset=[\"Disaster Subtype\"])\n",
    "\n",
    "#%29 u boş olan nümerik sutunun label ortalamasına göre doldurulması-Max değer ile diğer değerler çok farklı olduğundan normal ortalama kullanmadık-\n",
    "datadf['Total Deaths'].fillna(datadf.groupby('Disaster Subgroup')['Total Deaths'].transform('mean'),inplace=True)\n",
    "datadf=datadf.dropna(subset=[\"Total Deaths\"])\n",
    "\n",
    "#%29 u boş olan nümerik sutunun label ortalamasına göre doldurulması-Max değer ile diğer değerler çok farklı olduğundan normal ortalama kullanmadık-\n",
    "datadf['Total Affected'].fillna(datadf.groupby('Disaster Subgroup')['Total Affected'].transform('mean'),inplace=True)\n",
    "\n",
    "#%2 si boş olan nümerik sutunun ortalmaya göre doldurulması\n",
    "datadf[\"CPI\"].fillna(value={\"CPI\" :datadf[\"CPI\"].mean()},inplace=True)\n",
    "\n",
    "#3)Kategorik datanın nümeriğe çevrilmesi\n",
    "# Label olacak olan sutunu -Disaster Subgroup- Ordinal encoding ile sayılara çevrilmesi\n",
    "ord_enc = OrdinalEncoder()\n",
    "datadf[\"Target Value\"] = ord_enc.fit_transform(datadf[[\"Disaster Subgroup\"]])\n",
    "datadf.drop([\"Disaster Subgroup\"],axis=1,inplace=True)\n",
    "\n",
    "#Kategorik datamızda unique değer çok fazla olduğu için direkt olarak One-Hot-Encoding yapılmıyor bu sebeple grupları bir araya topladım.\n",
    "#Standart %5 ve altının yeni bir grupta toplanması ama ben bizim datamızın özelinde sutunlara göre yeni değerler belirledim.\n",
    "def combine(percent,name):\n",
    "    mask = datadf[name].map(datadf[name].value_counts()) < int(datadf.shape[0])*percent\n",
    "    datadf[name] =  datadf[name].mask(mask, 'other')\n",
    "\n",
    "combine(0.04,\"Disaster Type\")\n",
    "combine(0.04,\"Disaster Subtype\")\n",
    "combine(0.02,\"Country\")\n",
    "\n",
    "#One Hot Encoding ile kategorik data nümeriğe dönüştürülür\n",
    "def onehotencode(name):\n",
    "    encoder=ce.OneHotEncoder(cols=name,handle_unknown='return_nan',return_df=True,use_cat_names=True)\n",
    "    x = encoder.fit_transform(datadf)\n",
    "    return x\n",
    "\n",
    "datadf=onehotencode(\"Disaster Type\")\n",
    "datadf=onehotencode(\"Disaster Subtype\")\n",
    "datadf=onehotencode(\"Country\")\n",
    "datadf=onehotencode(\"Continent\")\n",
    "datadf=onehotencode(\"Season\")\n",
    "\n",
    "datadf = datadf.reset_index()\n",
    "\n",
    "#Modelin oluşturulması\n",
    "X=datadf[['Year', 'Seq', 'Disaster Type_Drought', 'Disaster Type_Earthquake',\n",
    "       'Disaster Type_other', 'Disaster Type_Storm', 'Disaster Type_Flood',\n",
    "       'Disaster Type_Epidemic', 'Disaster Type_Landslide',\n",
    "       'Disaster Subtype_Drought', 'Disaster Subtype_Ground movement',\n",
    "       'Disaster Subtype_other', 'Disaster Subtype_Tropical cyclone',\n",
    "       'Disaster Subtype_Riverine flood', 'Disaster Subtype_Bacterial disease',\n",
    "       'Disaster Subtype_Convective storm', 'Disaster Subtype_Flash flood',\n",
    "       'Country_other', 'Country_India', 'Country_Bangladesh', 'Country_China',\n",
    "       'Country_Indonesia', 'Country_United States of America (the)',\n",
    "       'Country_Japan', 'Country_Philippines (the)', 'Continent_Africa',\n",
    "       'Continent_Asia', 'Continent_Americas', 'Continent_Europe',\n",
    "       'Continent_Oceania', 'Season_Summer', 'Season_Spring', 'Season_Fall',\n",
    "       'Season_Winter', 'Total Deaths', 'Total Affected', 'CPI']]\n",
    "\n",
    "\n",
    "y=datadf[\"Target Value\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% training and 20% test\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(y_pred)\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "'''corrMatrix = data.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()'''\n",
    "'''data_crosstab = pd.crosstab(data[\"Region\"], data[\"Disaster Subgroup\"],  margins = False)\n",
    "print(data_crosstab)'''\"\"\"\"\n",
    "plt.bar(datadf[\"Disaster Subgroup\"], datadf[\"Disaster Type\"])\"\"\"\n",
    "\"\"\"plt.xlabel(\"Disaster Subgroup\", fontsize=8)\n",
    "plt.ylabel(\"Disaster Type\", fontsize=8)\"\"\"\n",
    "#plt.xticks(index, data_bar.province, fontsize=7, rotation=75)\n",
    "#4plt.title(‘Number of the confirmed COVID-19 Cases in South Korea’)\n",
    "\"\"\".show()\n",
    ".boxplot(x = 'day', y = 'total_bill', data = tips)\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNujKCam+FpOi+6D/WWW3qA",
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
